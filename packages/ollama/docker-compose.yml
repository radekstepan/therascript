# Defines the Ollama service container for the Therascript application.
# This file is typically managed *by* the Therascript backend (`packages/api`),
# not run directly unless for specific debugging or standalone use.

services:
  ollama:
    # Use the official Ollama image. ':latest' is convenient but consider pinning
    # a specific version for production stability (e.g., ollama/ollama:0.1.32).
    image: ollama/ollama:latest
    # Define a specific, predictable container name for easier management by other services/scripts.
    # Renamed slightly to avoid potential conflicts if an older 'ollama_server' exists.
    container_name: ollama_server_managed
    # Expose Ollama's default API port (11434) to the host machine.
    # Change the host port (e.g., "11435:11434") if 11434 is already in use on your host.
    # The backend service connects to the *host* port specified here.
    ports:
      - "11434:11434"
    # Use a named volume to persist downloaded models (/root/.ollama is the default location inside the container).
    # This prevents models from being lost when the container stops or restarts.
    volumes:
      - ollama_data:/root/.ollama
    # GPU Acceleration Configuration (Optional)
    # Uncomment this section if you have an NVIDIA GPU and the NVIDIA Container Toolkit installed
    # on the Docker host to significantly speed up model inference.
    # 'count: 1' allocates one GPU. Use 'all' to allocate all available GPUs.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 # Or 'all'
              capabilities: [gpu]
    # Healthcheck removed as it was problematic with GPU initialization times.
    # The backend service now handles readiness checks by polling the API.
    # Restart policy: Restart the container unless it was explicitly stopped.
    restart: unless-stopped

# Define the named volume used above for model persistence.
volumes:
  ollama_data:
