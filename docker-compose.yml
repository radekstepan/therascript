# Defines and orchestrates the Whisper and Elasticsearch service containers for the Therascript application.
# Ollama is managed by a separate compose file in `packages/ollama/docker-compose.yml`.

services:
  # Whisper Transcription Service
  whisper:
    build:
      # FIX: Change the build context to the project root directory '.'
      context: .
      # FIX: Specify the path to the Dockerfile relative to the new context
      dockerfile: packages/whisper/Dockerfile
    image: therascript/whisper
    container_name: therascript_whisper_service
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Voxtral vLLM OpenAI Server
  voxtral:
    build:
      context: .
      dockerfile: packages/voxtral/Dockerfile
    image: therascript/voxtral-proxy
    container_name: therascript_voxtral_service
    environment:
      - VOXTRAL_MODEL=mistralai/Voxtral-Mini-3B-2507
      - VOXTRAL_DTYPE=bfloat16
      - VOXTRAL_TOKENIZER_MODE=mistral
      - VOXTRAL_CONFIG_FORMAT=mistral
      - VOXTRAL_LOAD_FORMAT=mistral
      - VOXTRAL_MAX_MODEL_LEN=14384
      - VOXTRAL_GPU_MEMORY_UTILIZATION=0.90
      - VOXTRAL_CHUNK_SECONDS=300
      - VOXTRAL_AUDIO_BITRATE=32k
      - VOXTRAL_AUDIO_SAMPLE_RATE=16000
      - VOXTRAL_AUDIO_CHANNELS=1
    ports:
      - "127.0.0.1:8010:8000"
    volumes:
      - voxtral_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      # Proxy exposes /v1/models and forwards to the internal vLLM server
      test: ["CMD", "curl", "--fail", "http://localhost:8000/v1/models"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 120s

  # Elasticsearch Service
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.1 # Using a specific recent version
    container_name: therascript_elasticsearch_service
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m # Adjust RAM as needed for dev
      - xpack.security.enabled=false # Disable security for local dev (NOT for production)
      - TAKE_FILE_OWNERSHIP=true # For Docker volume permissions
    ulimits: # Recommended settings for Elasticsearch
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "127.0.0.1:9200:9200" # Elasticsearch API
      # - "127.0.0.1:9300:9300" # Elasticsearch transport (typically not needed for client)
    volumes:
      - es_data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -fsSL http://localhost:9200/_cat/health?h=status | grep -q 'green\\|yellow'"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 120s # Give ES more time to start up, especially first time

  # Kibana Service (Optional, for development and exploring ES data)
  kibana:
    image: docker.elastic.co/kibana/kibana:8.14.1 # Match Elasticsearch version
    container_name: therascript_kibana_service
    environment:
      - ELASTICSEARCH_HOSTS=http://therascript_elasticsearch_service:9200 # Connect to ES container
    ports:
      - "127.0.0.1:5601:5601"
    depends_on:
      elasticsearch:
        condition: service_healthy
    restart: unless-stopped

  # Redis Service for Job Queues
  redis:
    image: redis:7.2-alpine
    container_name: therascript_redis_service
    ports:
      - "127.0.0.1:6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  es_data: # Volume for Elasticsearch data persistence
    driver: local
  whisper_models:
    driver: local
  voxtral_cache:
    driver: local
  redis_data: # Volume for Redis data persistence
    driver: local
